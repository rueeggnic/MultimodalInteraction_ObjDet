{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\"\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image depicts a busy urban street scene. Various characters are engaged in different\n",
      "activities. In the foreground, a boy is sitting on the pavement looking at a device, while nearby, someone appears to be\n",
      "lying down. A group of pigeons is scattered around. On the sidewalk, there are people walking, including a woman\n",
      "carrying a phone and a man playing guitar. In the background, vehicles are moving, and there are buildings and a traffic\n",
      "light visible. The scene conveys a lively, bustling atmosphere typical of a city.', role='assistant',\n",
      "function_call=None, tool_calls=None, refusal=None, annotations=[])\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'scene': 'A bustling urban street scene',\n",
       "  'elements': {'background': {'buildings': 'A mix of modern skyscrapers and older brick buildings',\n",
       "    'traffic_lights': 'A traffic light showing red',\n",
       "    'vehicles': [{'type': 'car',\n",
       "      'color': 'silver',\n",
       "      'position': 'moving past the crosswalk'},\n",
       "     {'type': 'SUV', 'color': 'orange', 'position': 'in the intersection'},\n",
       "     {'type': 'motorcycle', 'position': 'in the crosswalk'}]},\n",
       "   'foreground': {'people': [{'action': 'sitting',\n",
       "      'position': 'on a bench',\n",
       "      'details': {'gender': 'female',\n",
       "       'clothing': 'red top and jeans',\n",
       "       'activity': 'reading a book'}},\n",
       "     {'action': 'sitting',\n",
       "      'position': 'on the ground',\n",
       "      'details': {'gender': 'male',\n",
       "       'clothing': 'red hoodie',\n",
       "       'activity': 'lying down'}},\n",
       "     {'action': 'standing',\n",
       "      'position': 'near the bench',\n",
       "      'details': {'gender': 'female',\n",
       "       'clothing': 'pink shirt and shorts',\n",
       "       'activity': 'looking at a phone'}},\n",
       "     {'action': 'walking',\n",
       "      'position': 'across the street',\n",
       "      'details': {'gender': 'male',\n",
       "       'clothing': 'black outfit',\n",
       "       'activity': 'playing guitar'}},\n",
       "     {'action': 'reading',\n",
       "      'position': 'sitting on the bench',\n",
       "      'details': {'gender': 'male',\n",
       "       'clothing': 'suit',\n",
       "       'activity': 'reading a newspaper'}}],\n",
       "    'animals': [{'type': 'pigeon', 'position': 'on the ground near the bench'},\n",
       "     {'type': 'pigeon', 'position': 'near the boy sitting on the ground'}],\n",
       "    'plants': {'type': 'flower pot',\n",
       "     'position': 'next to the boy',\n",
       "     'flowers': 'red flowers'}}},\n",
       "  'atmosphere': 'A lively city vibe with a mix of activities and interactions'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene': 'A bustling urban street scene',\n",
       " 'elements': {'background': {'buildings': 'A mix of modern skyscrapers and older brick buildings',\n",
       "   'traffic_lights': 'A traffic light showing red',\n",
       "   'vehicles': [{'type': 'car',\n",
       "     'color': 'silver',\n",
       "     'position': 'moving past the crosswalk'},\n",
       "    {'type': 'SUV', 'color': 'orange', 'position': 'in the intersection'},\n",
       "    {'type': 'motorcycle', 'position': 'in the crosswalk'}]},\n",
       "  'foreground': {'people': [{'action': 'sitting',\n",
       "     'position': 'on a bench',\n",
       "     'details': {'gender': 'female',\n",
       "      'clothing': 'red top and jeans',\n",
       "      'activity': 'reading a book'}},\n",
       "    {'action': 'sitting',\n",
       "     'position': 'on the ground',\n",
       "     'details': {'gender': 'male',\n",
       "      'clothing': 'red hoodie',\n",
       "      'activity': 'lying down'}},\n",
       "    {'action': 'standing',\n",
       "     'position': 'near the bench',\n",
       "     'details': {'gender': 'female',\n",
       "      'clothing': 'pink shirt and shorts',\n",
       "      'activity': 'looking at a phone'}},\n",
       "    {'action': 'walking',\n",
       "     'position': 'across the street',\n",
       "     'details': {'gender': 'male',\n",
       "      'clothing': 'black outfit',\n",
       "      'activity': 'playing guitar'}},\n",
       "    {'action': 'reading',\n",
       "     'position': 'sitting on the bench',\n",
       "     'details': {'gender': 'male',\n",
       "      'clothing': 'suit',\n",
       "      'activity': 'reading a newspaper'}}],\n",
       "   'animals': [{'type': 'pigeon', 'position': 'on the ground near the bench'},\n",
       "    {'type': 'pigeon', 'position': 'near the boy sitting on the ground'}],\n",
       "   'plants': {'type': 'flower pot',\n",
       "    'position': 'next to the boy',\n",
       "    'flowers': 'red flowers'}}},\n",
       " 'atmosphere': 'A lively city vibe with a mix of activities and interactions'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"] #[\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a 15-year-old male who is lying down and appears to be unconscious or resting, which indicates he may be in danger. The Child Hospital should be alerted due to the age of the individual.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, there is no mention of a 16-year-old individual in the image analysis. The closest age mentioned is 15 years old, who is described as \"lying down\" and \"unconscious or resting.\" \\n\\nIf you need to infer coordinates for the 15-year-old, you might consider their position relative to the other people in the scene. However, without specific coordinates or a visual reference, I cannot provide exact coordinates. \\n\\nIf you have a specific context or additional details about the layout of the scene, I could help you infer a possible location based on that information.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 16-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm unable to assist with that.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipywidgets --> Für Fortschrittsbalken, sonst wird ein Fehler ausgegeben\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) doesn't work in a single, unified way.  Instead, it encompasses a vast range of techniques and approaches, all aiming to create systems capable of performing tasks that typically require human intelligence.  Here's a breakdown of some key aspects:\n",
      "\n",
      "**1. Data is the Fuel:**  AI systems learn from data.  The more data they're trained on, the better they typically perform. This data can be anything from images and text to sensor readings and financial transactions.  The quality and quantity of data are crucial.\n",
      "\n",
      "**2. Algorithms are the Recipes:**  Algorithms are sets of rules and statistical techniques that AI systems use to process data and learn patterns.  Different algorithms are suited for different tasks.  Some common types include:\n",
      "\n",
      "* **Machine Learning (ML):**  This is a broad category where systems learn from data without explicit programming.  Instead of being explicitly programmed with rules, they identify patterns and make predictions based on the data they've processed.  Key subfields include:\n",
      "    * **Supervised Learning:**  The algorithm is trained on labeled data (e.g., images labeled as \"cat\" or \"dog\").  It learns to map inputs to outputs.\n",
      "    * **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find structure or patterns in the data.  Clustering and dimensionality reduction are examples.\n",
      "    * **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards or penalties for its actions.  This is often used in robotics and game playing.\n",
      "\n",
      "* **Deep Learning (DL):** A subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\").  These networks are inspired by the structure and function of the human brain and are particularly good at processing complex data like images, speech, and text.  Examples include convolutional neural networks (CNNs) for image recognition and recurrent neural networks (RNNs) for natural language processing.\n",
      "\n",
      "* **Expert Systems:** These systems use a set of rules defined by human experts to solve problems in a specific domain.  They are less adaptable than ML systems but can be effective in well-defined areas.\n",
      "\n",
      "**3. Models are the Representations:**  During training, the algorithm creates a model – a mathematical representation of the patterns it has learned from the data.  This model is then used to make predictions or decisions on new, unseen data.\n",
      "\n",
      "**4. Training is the Learning Process:**  Training an AI system involves feeding it large amounts of data and letting the algorithm adjust its internal parameters to improve its performance on a specific task.  This is an iterative process, and the model's accuracy is typically evaluated throughout.\n",
      "\n",
      "**5. Inference is the Application:**  Once trained, the AI model can be used to make inferences – predictions or decisions – on new data.  This is the stage where the AI system actually performs its task.\n",
      "\n",
      "\n",
      "**In simpler terms:** Imagine teaching a dog a trick.  You (the trainer) provide data (showing the dog the trick repeatedly), the algorithm (your training method) is how you teach it (positive reinforcement, repetition), the model is the dog's learned behavior, and inference is the dog performing the trick on command.\n",
      "\n",
      "\n",
      "AI is a constantly evolving field, and new techniques and approaches are continually being developed. This explanation provides a general overview; the specifics of how a particular AI system works depend heavily on its design and the task it's intended to perform.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[698,326,964,626]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
